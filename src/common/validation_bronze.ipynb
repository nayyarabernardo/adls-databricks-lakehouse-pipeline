{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "246825da-362e-48af-90e7-e7897fdd3647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from src.common.utils import load_json\n",
    "from src.config.settings import METADATA_BASE_PATH, LAYER_PATHS\n",
    "from pyspark.sql.utils import AnalysisException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbd3e53-6903-48c1-8f04-43da7f99cb89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "dbutils.widgets.text(\"table_name\", \"\")\n",
    "\n",
    "table_name = dbutils.widgets.get(\"table_name\") or None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e73f246c-a69b-46fb-996b-f5c6ef52bcd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def validate_table_exists(path: str):\n",
    "    try:\n",
    "        spark.read.format(\"delta\").load(path)\n",
    "    except AnalysisException:\n",
    "        raise Exception(f\"Tabela não encontrada: {path}\")\n",
    "\n",
    "\n",
    "def validate_not_empty(df, table_name: str):\n",
    "    if df.limit(1).count() == 0:\n",
    "        raise Exception(f\"Tabela {table_name} está vazia\")\n",
    "\n",
    "\n",
    "def validate_delta_format(path: str):\n",
    "    if not spark._jsparkSession.sessionState().catalog().tableExists(path):\n",
    "        # fallback seguro\n",
    "        try:\n",
    "            spark.read.format(\"delta\").load(path)\n",
    "        except Exception:\n",
    "            raise Exception(f\"Tabela não está em formato Delta: {path}\")\n",
    "\n",
    "\n",
    "def validate_required_columns(df, required_columns: list, table_name: str):\n",
    "    missing = [c for c in required_columns if c not in df.columns]\n",
    "    if missing:\n",
    "        raise Exception(\n",
    "            f\"Tabela {table_name} está sem colunas obrigatórias: {missing}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41bec18f-0aa9-4c60-ba77-e0c138eaf96b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "metadata_path = f\"{METADATA_BASE_PATH}/bronze\"\n",
    "\n",
    "files_df = spark.read.format(\"binaryFile\") \\\n",
    "    .load(f\"{metadata_path}/*.json\")\n",
    "\n",
    "json_files = [row.path for row in files_df.select(\"path\").collect()]\n",
    "\n",
    "errors = []\n",
    "\n",
    "for file_path in json_files:\n",
    "    metadata = load_json(file_path)\n",
    "    table = metadata[\"table_name\"]\n",
    "\n",
    "    if table_name and table != table_name:\n",
    "        continue\n",
    "\n",
    "    print(f\"Validando tabela bronze: {table}\")\n",
    "\n",
    "    table_path = f\"{LAYER_PATHS['bronze']}/{table}\"\n",
    "\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(table_path)\n",
    "\n",
    "        validate_not_empty(df, table)\n",
    "        validate_required_columns(\n",
    "            df,\n",
    "            metadata.get(\"required_columns\", []),\n",
    "            table\n",
    "        )\n",
    "\n",
    "        print(f\"{table} OK\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        errors.append(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cab319-82bb-4b94-ac78-00f678e9c797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if errors:\n",
    "    raise Exception(\n",
    "        \"Validação da camada bronze falhou:\\n\" + \"\\n\".join(errors)\n",
    "    )\n",
    "\n",
    "print(\"Validação da camada bronze concluída com sucesso\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "validation_bronze",
   "widgets": {
    "table_name": {
     "currentValue": "order_items",
     "nuid": "b52f5fd3-eb1c-4ef3-947f-50dcec9db224",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
